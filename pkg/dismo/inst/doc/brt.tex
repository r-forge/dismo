\documentclass{article}

\usepackage{natbib}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage{hanging}
\usepackage{hyperref}

% \VignetteIndexEntry{Boosted Regression Trees for ecological modeling}
% \VignetteDepends{dismo}
% \VignetteKeyword{spatial}


\usepackage{Sweave}
\begin{document}


\title{Boosted Regression Trees for ecological modeling}

\author{Jane Elith and John Leathwick}

\maketitle

\section{Introduction}

This is a brief tutorial to accompany a set of functions that we have written to facilitate fitting BRT (boosted regression tree) models in R.  This tutorial is a modified version of the tutorial accompaniying Elith, Leathwick and Hastie's article in Journal of Animal Ecology. It has been adjusted to match the implementation of these functions in the 'dismo' package. The gbm* functions in the dismo package extend functions in the 'gbm' package by Greg Ridgeway. The goal of our functions is to make the functions in the 'gbm' package easier to apply to ecological data, and to enhance interpretation. 

The tutorial is aimed at helping you to learn the mechanics of how to use the functions and to develop a BRT model in R. It does not explain what a BRT model is - for that, see the references at the end of the tutorial, and the documentation of the gbm package. For an example application with similar data as in this tutorial, see Elith et al., 2008. 

The gbm functions in 'dismo' are as follows:

1. gbm.step - Fits a gbm model to one or more response variables, using cross-validation to estimate the optimal number of trees. This requires use of the utility functions roc, calibration and calc.deviance. 

2. gbm.fixed, gbm.holdout - Alternative functions for fitting gbm models, implementing options provided in the gbm package.

3. gbm.simplify - Code to perform backwards elimination of variables, to drop those that give no evidence of improving predictive performance.

4. gbm.plot - Plots the partial dependence of the response on one or more predictors.

5. gbm.plot.fits - Plots the fitted values from a gbm object returned by any of the model fitting options. This can give a more reliable guide to the shape of the fitted surface than can be obtained from the individual functions, particularly when predictor variables are correlated and/or samples are unevenly distributed in environmental space. 

6. gbm.interactions - Tests whether interactions have been detected and modelled, and reports the relative strength of these. Results can be visualised with gbm.perspec 


\section{Example data}

Two sets of presence/absence data for Anguilla australis (Angaus) are available. One for model training (building) and one for model testing (evaluation). In the example below we load the training data. Presence (1) and absence (0) is recorded in column 2. The environmental variables are in columns 3 to 14. This is the same data as used in Elith, Leathwick and Hastie (2008). 


\begin{Schunk}
\begin{Sinput}
> library(dismo)
\end{Sinput}
\begin{Soutput}
raster version 1.6-15 (7-November-2010)
\end{Soutput}
\begin{Sinput}
> data(Anguilla_train)
> head(Anguilla_train)
\end{Sinput}
\begin{Soutput}
  Site Angaus SegSumT SegTSeas SegLowFlow DSDist DSMaxSlope USAvgT USRainDays
1    1      0    16.0    -0.10      1.036  50.20       0.57   0.09      2.470
2    2      1    18.7     1.51      1.003 132.53       1.15   0.20      1.153
3    3      0    18.3     0.37      1.001 107.44       0.57   0.49      0.847
4    4      0    16.7    -3.80      1.000 166.82       1.72   0.90      0.210
5    5      1    17.2     0.33      1.005   3.95       1.15  -1.20      1.980
6    6      0    15.1     1.83      1.015  11.17       1.72  -0.20      3.300
  USSlope USNative DSDam   Method LocSed
1     9.8     0.81     0 electric    4.8
2     8.3     0.34     0 electric    2.0
3     0.4     0.00     0      spo    1.0
4     0.4     0.22     1 electric    4.0
5    21.9     0.96     0 electric    4.7
6    25.7     1.00     0 electric    4.5
\end{Soutput}
\begin{Sinput}
> Anguilla_train = Anguilla_train[1:250, ]
\end{Sinput}
\end{Schunk}


\section{Fitting a model}

To fit a gbm model, you need to decide what settings to use the article associated with this tutorial gives you information on what to use as rules of thumb. These data have 1000 sites, comprising 202 presence records for the short-finned eel (the command sum(model.data\$Angaus)will give you the total number of presences). As a first guess you could decide:
1. There are enough data to model interactions of reasonable complexity 
2. A lr of about 0.01 could be a reasonable starting point. 

To below example shows how to use our function that steps forward and identifies the optimal number of trees (nt).

\begin{Schunk}
\begin{Sinput}
> angaus.tc5.lr01 <- gbm.step(data = Anguilla_train, gbm.x = 3:13, 
+     gbm.y = 2, family = "bernoulli", tree.complexity = 5, learning.rate = 0.01, 
+     bag.fraction = 0.5)
\end{Sinput}
\begin{Soutput}
Loaded gbm 1.6-3.1 

 
 GBM STEP - version 2.9 
 
Performing cross-validation optimisation of a boosted regression tree model 
for Angaus with dataframe Anguilla_train and using a family of bernoulli 
Using 250 observations and 11 predictors 
creating 10 initial models of 50 trees 

 folds are stratified by prevalence 
total mean deviance =  1.0436 
tolerance is fixed at  0.001 
ntrees resid. dev. 
50    0.863 
now adding trees... 
100   0.7725 
150   0.7314 
200   0.7049 
250   0.6896 
300   0.6823 
350   0.6799 
400   0.6804 
450   0.6853 
500   0.6934 
550   0.6964 
600   0.7034 
650   0.7077 
700   0.7156 
750   0.7212 
800   0.7275 
850   0.7331 
900   0.7392 
950   0.7471 
1000   0.7551 
fitting final gbm model with a fixed number of  350  trees for  Angaus 

mean total deviance = 1.044 
mean residual deviance = 0.365 
 
estimated cv deviance = 0.68 ; se = 0.064 
 
training data correlation = 0.871 
cv correlation =  0.622 ; se = 0.053 
 
training data ROC score = 0.99 
cv ROC score = 0.883 ; se = 0.023 
 
elapsed time -  0.1 minutes 
\end{Soutput}
\end{Schunk}
\includegraphics{brt-002}

Above we used the function gbm.step, this function is an alternative to the cross-validation provided in the gbm package.  
We have passed information to the function about data and settings. We have defined:

the dataframe containing the data = Anguilla\_train;

the predictor variables - gbm.x = c(3:13) -  which we do using a vector consisting of the indices for the data columns containing the predictors (i.e., here the predictors are columns 3 to 13 in Anguilla\_train);

the response variable - gbm.y = 2 -  indicating the column number for the species (response) data;

the nature of the error structure - For example,  family = 'bernoulli' (note the quotes); 

the tree complexity - we are trying a tree complexity of 5 for a start;

the learning rate - we are trying with 0.01;

the bag fraction - our default is 0.75; here we are using 0.5;

Everything else - i.e. all the other things that we could change if we wanted to (see the help file, and the documentation of the gbm package) - are set at their defaults if they are not named in the call. If you want to see what else you could change, you can type gbm.step and all the code will write itself to screen, or type args(gbm.step) and it will open in an editor window.

Running a model such as that described above writes progress reports to the screen, makes a graph, and returns an object containing a number of components. Firstly, the things you can see: The R console will show something like this (not identical, because remember that these models are stochastic and therefore slightly different each time you run them, unless you set the seed or make them deterministic by using a bag fraction of 1)

This reports a brief model summary. All these values are also retained in the model object, so they will be permanently kept (as long as you save the R workspace before quitting).

This model was built with the default 10-fold cross-validation. The solid black curve is the mean, and the dotted curves about 1 standard error, for the changes in predictive deviance (ie as measured on the excluded folds of the cross-validation). The red line shows the minimum of the mean, and the green line the number of trees at which that occurs. The final model that is returned in the model object is built on the full data set, using the number of trees identified as optimal. 

The returned object is a list (see R documentation if you don't know what that is), and the names of the components can be seen by typing:

To pull out one component of the list, use a number (angaus.tc5.lr01[[29]]) or name (angaus.tc5.lr01\$cv.statistics) - but be careful, some are as big as the dataset, e.g. there will be 1000 fitted values. Find this by typing 
length(angaus.tc5.lr01\$fitted)

The way we organise our functions is to return exactly what Ridgeway's function in the gbm package returned, plus extra things that are relevant to our code. You will see by looking at the final parts of the gbm.step code that we have added components 25 onwards, that is, from gbm.call on. See the gbm documentation for what his parts comprise. Ours are:

gbm.call - A list containing the details of the original call to gbm.step
 
fitted - The fitted values from the final tree, on the response scale
 
fitted.vars - The variance of the fitted values, on the response scale

residuals - The residuals for the fitted values, on the response scale 

contributions - The relative importance of the variables, produced from the gbm summary function

self.statistics - The relevant set of evaluation statistics, calculated on the fitted values - i.e. this is only interesting in so far as it demonstrates "evaluation" (i.e. fit) on the training data. It should NOT be reported as the model predictive performance.

cv.statistics These are the most appropriate evaluation statistics. We calculate each statistic within each fold (at the identified optimal number of trees that is calculated on the mean change in predictive deviance over all folds), then present here the mean and standard error of those fold-based statistics. 

weights - the weights used in fitting the model (by default, "1" for each observation - i.e. equal weights). 

trees.fitted - A record of the number of trees fitted at each step in the stagewise fitting; only relevant for later calculations

training.loss.values - The stagewise changes in deviance on the training data 

cv.values - the mean of the CV estimates of predictive deviance, calculated at each step in the stagewise process - this and the next are used in the plot shown above

cv.loss.ses - standard errors in CV estimates of predictive deviance at each step in the stagewise process

cv.loss.matrix - the matrix of values from which cv.values were calculated - as many rows as folds in the CV

cv.roc.matrix - as above, but the values in it are area under the curve estimated on the excluded data, instead of deviance in the cv.loss.matrix.

You can look at variable importance with the summary function
\begin{Schunk}
\begin{Sinput}
> names(angaus.tc5.lr01)
\end{Sinput}
\begin{Soutput}
 [1] "initF"                "fit"                  "train.error"         
 [4] "valid.error"          "oobag.improve"        "trees"               
 [7] "c.splits"             "bag.fraction"         "distribution"        
[10] "interaction.depth"    "n.minobsinnode"       "n.trees"             
[13] "nTrain"               "response.name"        "shrinkage"           
[16] "train.fraction"       "var.levels"           "var.monotone"        
[19] "var.names"            "var.type"             "verbose"             
[22] "data"                 "Terms"                "cv.folds"            
[25] "gbm.call"             "fitted"               "fitted.vars"         
[28] "residuals"            "contributions"        "self.statistics"     
[31] "cv.statistics"        "weights"              "trees.fitted"        
[34] "training.loss.values" "cv.values"            "cv.loss.ses"         
[37] "cv.loss.matrix"       "cv.roc.matrix"       
\end{Soutput}
\begin{Sinput}
> summary(angaus.tc5.lr01)
\end{Sinput}
\begin{Soutput}
          var   rel.inf
1     SegSumT 20.953170
2    USNative 16.284780
3      Method 13.907519
4      DSDist 10.477186
5  SegLowFlow  7.735423
6  DSMaxSlope  6.983738
7    SegTSeas  6.912201
8     USSlope  6.196858
9      USAvgT  6.110745
10 USRainDays  4.438381
11      DSDam  0.000000
\end{Soutput}
\end{Schunk}
\includegraphics{brt-003}

\section{Choosing the settings}

The above was a first guess at settings, using rules of thumb discussed in Elith et al. (2008). It made a model with only 650 trees, so our next step would be to reduce the lr. For example, try lr = 0.005, to aim for over 1000 trees:
 
\begin{Schunk}
\begin{Sinput}
> angaus.tc5.lr005 <- gbm.step(data = Anguilla_train, gbm.x = 3:13, 
+     gbm.y = 2, family = "bernoulli", tree.complexity = 5, learning.rate = 0.005, 
+     bag.fraction = 0.5)
\end{Sinput}
\begin{Soutput}
 GBM STEP - version 2.9 
 
Performing cross-validation optimisation of a boosted regression tree model 
for Angaus with dataframe Anguilla_train and using a family of bernoulli 
Using 250 observations and 11 predictors 
creating 10 initial models of 50 trees 

 folds are stratified by prevalence 
total mean deviance =  1.0436 
tolerance is fixed at  0.001 
ntrees resid. dev. 
50    0.9435 
now adding trees... 
100   0.8799 
150   0.8337 
200   0.8064 
250   0.7872 
300   0.7701 
350   0.759 
400   0.7516 
450   0.7456 
500   0.7457 
550   0.7456 
600   0.7447 
650   0.7462 
700   0.7501 
750   0.7507 
800   0.7551 
850   0.7557 
900   0.7554 
950   0.7579 
1000   0.761 
1050   0.7674 
1100   0.771 
1150   0.7775 
fitting final gbm model with a fixed number of  600  trees for  Angaus 

mean total deviance = 1.044 
mean residual deviance = 0.396 
 
estimated cv deviance = 0.745 ; se = 0.073 
 
training data correlation = 0.857 
cv correlation =  0.558 ; se = 0.077 
 
training data ROC score = 0.986 
cv ROC score = 0.845 ; se = 0.042 
 
elapsed time -  0.1 minutes 
\end{Soutput}
\end{Schunk}
\includegraphics{brt-004}

To more broadly explore whether other settings perform better, and assuming that these are the only data available, you could either split the data into a training and testing set or use the cross-validation results. You could systematically alter tc,  lr and the bag fraction and compare the results. See the later section on prediction to find out how to predict to independent data and calculate relevant statistics. 


\section{Alternative ways to fit models}

The step function above is slower than just fitting one model and finding a minimum.  If this is a problem, you could use our gbm.holdout code - this combines from the gbm package in ways we find useful.  We tend to prefer gbm.step, especially when modelling many species, because it automatically finds the optimal number of trees.  Alternatively, the gbm.fixed code allows you to fit a model of a set number of trees; this can be used, as in Elith et al. (2008), to predict to new data (see later section).


\section{Simplifying the model}

For a discussion of simplification see Appendix 2 of the online supplement to Elith et al (2008). Simplification builds many models, so it can be slow. For example, the code below took a few minutes to run on a modern laptop. In it we assess the value in simplifying the model built with a lr of 0.005, but only test dropping up to 5 variables (the "n.drop" argument; the default is an automatic rule so it continues until the average change in predictive deviance exceeds its original standard error as calculated in gbm.step).

\begin{Schunk}
\begin{Sinput}
> angaus.simp <- gbm.simplify(angaus.tc5.lr005, n.drops = 5)